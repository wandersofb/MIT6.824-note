# Google File System

## 介绍

GFS 是一个分布式文件系统，他和以前的分布式文件系统有着相同的目标：性能、可扩展性和可靠性。它的设计是由于 google的应用工作负载和技术环境的影响。举例：
- 组件错误是常态。因此，持续监测、错误检测、容错和自动回复必须成为系统的一部分。
- 文件大小是巨大的，通常在 GB 甚至 TB 大小。必须重新审视设计的假设和参数，例如 I/O 和 block size。
- 大多数文件是以通过添加新的数据而不是覆盖现有的数据来突变的。文件内部的随机写入是不存在。一旦写入，文件只被可读，而且是按顺序读取的。鉴于这种对巨大文件的访问模式，追加写入成为性能优化和原子性保证的焦点，而在客户端缓存数据块则失去了吸引力。

## 设计概述

### 假设

- 系统是被建立在那些非昂贵并且经常发生故障的商品组件上。它必须持续的监视、检查、容忍和恢复。
- 系统存储适中的大文件，预计有几百万个文件，每个文件有 100 MB 甚至更大。当然必须支持小文件，但不需要为他优化。
- 工作负载主要包括俩个部分：大型流式读取和小型随机读取。
- 工作负载还有许多将数据附加到文件的大型顺序写入。支持文件中任意位置的小写操作，但不一定要高效。
- 系统必须为多写文件实现高效的同步语义。
- 高持续带宽比低延迟更重要。

### 接口

GFS 提供了许多熟悉的文件系统接口，例如：*create*, *delete*, *open*, *close*, *read* and *write*。
此外，GFS 提供了 *snapshot* 和 *record append* 操作。Snapshot 提供以低成本创建文件或目录的副本。Record append 在保证原子性的前提下允许多个客户端同时将数据追加到同一个文件。

### 架构

GFS 集群是由一个 *master* 和许多 *chunkservers* 组成的，并由多个客户端访问。每个客户端通常都是运用户级服务器进程的 Linux 机器。
文件被分成固定大小的 *chunk*。每个 chunk 都是由 master 在创建 chunk 时分配的不可变且全局唯一64位 *chunk handle*标识的。
Chunkservers 将 chunk 作为 Linux 文件在本地磁盘存储，并读取或写入由 chunk handle 和 byte range 指定的 chunk data。
Master 维护所有文件系统的元数据。这包括命名空间、访问控制信息、从文件到块的映射以及块的当前位置。它还控制系统范围的活动，例如块租用管理、孤立块的垃圾收集以及块服务器之间的块迁移。Master 周期性地在 *HeartBeat* 消息中与每个 chunkserver 通信，给它指令并收集它的状态。
链接到每个应用程序的 GFS 客户端代码实现文件系统 API 并与主服务器和块服务器通信以代表应用程序读取或写入数据。 客户端与 master 交互以进行元数据操作，但所有数据承载通信直接进入 chunkservers。我们不提供 POSIX API，因此不需要连接到 Linux vnode 层。
客户端和 chunkserver 都不缓存文件数据。客户端缓存几乎没有什么好处，因为大多数应用程序流式传输大量文件或工作集太大而无法缓存。没有它们可以通过消除缓存一致性问题来简化客户端和整个系统。（然而，客户端会缓存元数据。）Chunkservers 不需要缓存文件数据，因为块存储为本地文件，因此 Linux 的缓冲区缓存已经将经常访问的数据保存在内存中。

![](../.gitbook/assets/GFS%20Architecture.png)

### Single Master

拥有一个单一的 master 极大地简化了我们的设计，并使 master 能够利用全局消息做出复杂的块放置和复制决策。
客户端永远不会通过 master 读写文件数据。相反，客户端询问主服务器它应该联系哪些块服务器。 它会在有限的时间内缓存此信息，并直接与 chunkservers 交互以进行许多后续操作。
参考图一简单解释一下读写：
- 首先，使用固定块大小，客户端将应用程序指定的文件名和字节偏移量转换为文件内的块索引。
- 然后，它向 master 发送一个包含文件名和块索引的请求。
- Master 回复相应的块句柄和副本的位置。
- 客户端使用文件名和块索引作为键来缓存此信息。然后客户端向其中一个副本发送请求，最有可能是最近的副本。该请求指定块句柄和该块内的字节范围。在缓存信息过期或文件重新打开之前，对同一块的进一步读取不需要更多的客户端与主机交互。事实上，客户端通常会在同一个请求中请求多个块，而主服务器也可以包含紧跟在请求之后的块的信息。这些额外的信息几乎无需额外费用即可回避未来几次客户与主人的互动。

### Chunk Size

GFS 选择了 64MB 作为块大小，这比经典的文件系统的块大小都要大很多。

巨大的块大小有很多优势：
- 首先，它减少了客户端与 master 交互的需要，因为对同一块的读写只需要 master 发出一次初始请求以获取块的位置信息。
- 其次，由于在大块上客户端更有可能在给定块上执行许多操作，因此它可以通过在延长的时间段内保持与 chunkserver 的持久 TCP 连接来减少网络开销。
-  第三，它减少了存储在 master 上的元数据的大小。

当然也有缺点，一个小文件由少量块组成，也许只有一个。如果许多客户端访问同一个文件，存储这些块的 chunkservers 可能会成为热点。实际上，热点并不是主要问题，因为我们的应用程序大多按顺序读取大型多块文件。
然而，当 GFS 首次被批处理队列系统使用时，热点确实出现了：一个可执行文件作为单块文件写入 GFS，然后同时在数百台机器上启动。存储此可执行文件的少数 chunkservers 因数百个同时请求而超载。我们通过以更高的复制因子存储此类可执行文件并使批处理队列系统错开应用程序启动时间来解决此问题。一个潜在的长期解决方案是允许客户端在这种情况下从其他客户端读取数据。

### Metadata

Master 存储三种主要类型的元数据：文件和块命名空间、从文件到块的映射以及每个块的副本的位置。所有元数据都保存在 master 的内存中。前两种类型（命名空间和文件到块的映射）也通过将突变记录到存储在主服务器本地磁盘上并复制到远程机器上的 *operation log* 中来保持持久性。 使用日志可以让我们简单、可靠地更新主节点状态，并且不会在主节点崩溃时冒不一致的风险。  master 不会持久存储块位置信息。 相反，它会在 master 启动时以及每当一个 chunkserver 加入集群时向每个 chunkserver 询问它的 chunks。

### Consistency Model

GFS 的一致性模型相对宽松，很好的支持高度分布式的应用程序。

#### Guarantees by GFS

![](../.gitbook/assets/File%20Region%20State%20After%20Mutation.png)

